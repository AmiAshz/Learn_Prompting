"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[5470],{85194:(e,a,o)=>{o.r(a),o.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>g,frontMatter:()=>p,metadata:()=>m,toc:()=>c});var t=o(87462),n=(o(67294),o(3905));const r=o.p+"assets/images/zero_shot-1af9e1cb88412f9fdefa3b07b67c4193.png",s=o.p+"assets/images/zero_shot_example-89065990663d4ef044011844ff77f9af.png";var i=o(39145);const p={sidebar_position:4},l="\ud83d\udfe2 Zero Shot Chain of Thought",m={unversionedId:"intermediate/zero_shot_cot",id:"intermediate/zero_shot_cot",title:"\ud83d\udfe2 Zero Shot Chain of Thought",description:'Zero Shot Chain of Thought (Zero-shot-CoT) prompting (@kojima2022large) es un seguimiento del %%prompting de CoT|prompting de CoT%% (@wei2022chain), que introduce un prompt zero shot incre\xedblemente simple. Descubren que al agregar las palabras "Pensemos paso a paso." al final de una pregunta, los LLM pueden generar una cadena de pensamiento que responde a la pregunta. A partir de esta cadena de pensamiento, pueden extraer respuestas m\xe1s precisas.',source:"@site/i18n/es/docusaurus-plugin-content-docs/current/intermediate/zero_shot_cot.md",sourceDirName:"intermediate",slug:"/intermediate/zero_shot_cot",permalink:"/es/docs/intermediate/zero_shot_cot",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v1.2.3/docs/intermediate/zero_shot_cot.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udfe2 Cadena de pensamiento",permalink:"/es/docs/intermediate/chain_of_thought"},next:{title:"\ud83d\udfe1 Autoconsistencia",permalink:"/es/docs/intermediate/self_consistency"}},d={},c=[{value:"Ejemplo",id:"ejemplo",level:2},{value:"Incorrecta",id:"incorrecta",level:4},{value:"Correcta",id:"correcta",level:4},{value:"Resultados",id:"resultados",level:2},{value:"Ablaciones de Inter\xe9s",id:"ablaciones-de-inter\xe9s",level:2},{value:"Notas",id:"notas",level:2}],u={toc:c},h="wrapper";function g(e){let{components:a,...o}=e;return(0,n.kt)(h,(0,t.Z)({},u,o,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"-zero-shot-chain-of-thought"},"\ud83d\udfe2 Zero Shot Chain of Thought"),(0,n.kt)("p",null,"Zero Shot Chain of Thought (Zero-shot-CoT) prompting",(0,n.kt)("sup",{parentName:"p",id:"fnref-1"},(0,n.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))," es un seguimiento del ",(0,n.kt)("a",{parentName:"p",id:"prompting de CoT_2_23_1689000791983","data-tooltip-html":"La idea principal de CoT es que al mostrarle al LLM algunos ejemplos de few-shot donde se explica el proceso de razonamiento en los ejemplos, el LLM tambi\xe9n mostrar\xe1 el proceso de razonamiento al responder a la solicitud.","data-tooltip-place":"top"},"prompting de CoT"),(0,n.kt)(i.u,{anchorId:"prompting de CoT_2_23_1689000791983",clickable:!0,mdxType:"Tooltip"}),(0,n.kt)("sup",{parentName:"p",id:"fnref-2"},(0,n.kt)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2")),', que introduce un prompt zero shot incre\xedblemente simple. Descubren que al agregar las palabras "',(0,n.kt)("strong",{parentName:"p"},"Pensemos paso a paso."),'" al final de una pregunta, los LLM pueden generar una cadena de pensamiento que responde a la pregunta. A partir de esta cadena de pensamiento, pueden extraer respuestas m\xe1s precisas.'),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)("img",{src:r,style:{width:"500px"}})),(0,n.kt)("div",{style:{textAlign:"center"}},"Zero Shot CoT (Kojima et al.)"),(0,n.kt)("p",null,"T\xe9cnicamente, el proceso completo de Zero-shot-CoT implica dos prompts/completions separados. En la siguiente imagen, la burbuja superior de la izquierda genera una cadena de pensamiento, mientras que la burbuja superior de la derecha toma la salida del primer prompt (incluido el primer prompt en s\xed mismo) y extrae la respuesta de la cadena de pensamiento. Este segundo prompt es un prompt ",(0,n.kt)("em",{parentName:"p"},"self augmented"),"."),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)("img",{src:s,style:{width:"500px"}})),(0,n.kt)("div",{style:{textAlign:"center"}},"Full Zero Shot CoT Process (Kojima et al.)"),(0,n.kt)("h2",{id:"ejemplo"},"Ejemplo"),(0,n.kt)("p",null,"Aqu\xed hay algunas demostraciones (que solo realizan extracci\xf3n de razonamiento). Esta primera demostraci\xf3n muestra a GPT-3 (davinci-003) fallando en una simple pregunta de matem\xe1ticas, mientras que la segunda demostraci\xf3n utiliza un prompt Zero-shot-CoT y resuelve el problema con \xe9xito. Si\xe9ntase libre de ingresar su clave de API de OpenAI (haga clic en Generar) y jugar con los ejemplos. Tenga en cuenta lo mucho m\xe1s simple que es el prompt Zero-shot-CoT en comparaci\xf3n con el prompt CoT."),(0,n.kt)("h4",{id:"incorrecta"},"Incorrecta"),(0,n.kt)("div",{"trydyno-embed":"","openai-model":"text-davinci-003","initial-prompt":"Si John tiene 5 peras, luego come 2, y compra 5 m\xe1s, luego le da 3 a su amigo, \xbfcu\xe1ntas peras tiene?","initial-response":"John tiene 8 peras.","max-tokens":"256","box-rows":"3","model-temp":"0.7","top-p":"1"}),(0,n.kt)("h4",{id:"correcta"},"Correcta"),(0,n.kt)("div",{"trydyno-embed":"","openai-model":"text-davinci-003","initial-prompt":"Si John tiene 5 peras, luego come 2, y compra 5 m\xe1s, luego le da 3 a su amigo, \xbfcu\xe1ntas peras tiene?\\n\\nPensemos paso a paso.","initial-response":"John comienza con 5 peras. Come 2 peras, lo que lo deja con 3 peras. Compra 5 peras m\xe1s, lo que le da un total de 8 peras. Le da 3 peras a su amigo, lo que lo deja con solo 5 peras.","max-tokens":"256","box-rows":"5","model-temp":"0.7","top-p":"1"}),(0,n.kt)("h2",{id:"resultados"},"Resultados"),(0,n.kt)("p",null,"Zero-shot-CoT tambi\xe9n fue efectivo para mejorar los resultados en tareas de aritm\xe9tica,\nsentido com\xfan y razonamiento simb\xf3lico. Sin embargo, como era de esperar, generalmente\nno fue tan efectivo como el CoT prompting. Un caso de uso importante para Zero-shot-CoT\nes cuando es dif\xedcil obtener ejemplos de few shot para la generaci\xf3n de CoT."),(0,n.kt)("h2",{id:"ablaciones-de-inter\xe9s"},"Ablaciones de Inter\xe9s"),(0,n.kt)("p",null,'Kojima et al. experimentaron con varios prompts de Zero-shot-CoT diferentes (por ejemplo, "Resolvamos este problema dividi\xe9ndolo en pasos." o "Pensemos en esto l\xf3gicamente."), pero encontraron que "Pensemos paso a paso" es el m\xe1s efectivo para las tareas que seleccionaron.'),(0,n.kt)("h2",{id:"notas"},"Notas"),(0,n.kt)("p",null,"El paso de extracci\xf3n a menudo debe ser espec\xedfico de la tarea, lo que hace que Zero-Shot-CoT sea menos generalizable de lo que parece al principio."),(0,n.kt)("p",null,"Anecd\xf3ticamente, he encontrado que los prompts de estilo Zero-shot-CoT a veces son efectivos para mejorar la longitud de las completaciones para tareas generativas. Por ejemplo, considera el prompt est\xe1ndar ",(0,n.kt)("inlineCode",{parentName:"p"},"Escribe una historia sobre una rana y un hongo que se hacen amigos"),". Agregar las palabras ",(0,n.kt)("inlineCode",{parentName:"p"},"Pensemos paso a paso.")," al final de este prompt conduce a una completaci\xf3n mucho m\xe1s larga."),(0,n.kt)("div",{className:"footnotes"},(0,n.kt)("hr",{parentName:"div"}),(0,n.kt)("ol",{parentName:"div"},(0,n.kt)("li",{parentName:"ol",id:"fn-1"},"Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners.\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-2"},"Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models.\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-2",className:"footnote-backref"},"\u21a9")))))}g.isMDXComponent=!0}}]);